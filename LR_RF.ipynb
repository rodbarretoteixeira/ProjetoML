{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functionsML as f\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b182c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de Limpeza Básica (Sem Leakage)\n",
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop irrelevant\n",
    "    df = df.drop(columns=[\"hasDamage\",\"paintQuality%\"], errors='ignore')\n",
    "    \n",
    "    # Text handling\n",
    "    text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    df[text_cols] = df[text_cols].apply(lambda x: x.str.lower() if x.dtype==\"object\" else x)\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df = f.fix_typos(col, df)\n",
    "\n",
    "    # Transmission: 'other' passa a 'unknown'\n",
    "    df['transmission'] = df['transmission'].replace('other', 'unknown')\n",
    "    \n",
    "    # FuelType: 'other' passa a 'electric' (conforme o teu código antigo)\n",
    "    df['fuelType'] = df['fuelType'].replace('other', 'electric')\n",
    "\n",
    "    # Filtering / Cleaning Rules\n",
    "    df.loc[df[\"mileage\"] < 0, \"mileage\"] = np.nan\n",
    "    df.loc[~df[\"tax\"].between(0, 600), \"tax\"] = np.nan\n",
    "    df.loc[~df[\"mpg\"].between(0, 150), \"mpg\"] = np.nan\n",
    "    df.loc[~df[\"engineSize\"].between(1, 6.3), \"engineSize\"] = np.nan\n",
    "    df.loc[~df[\"year\"].between(1990, 2020), \"year\"] = np.nan\n",
    "    df.loc[~df[\"previousOwners\"].between(0, 6), \"previousOwners\"] = np.nan # Opcional conforme o teu código\n",
    "\n",
    "    # Numeric Transformations\n",
    "    df['mileage'] = np.log1p(df['mileage'])\n",
    "    df['mpg'] = np.log1p(df['mpg'])\n",
    "    df['tax'] = np.log1p(df['tax'])\n",
    "    \n",
    "    # Types and Rounding\n",
    "    df[\"year\"] = df[\"year\"].round()\n",
    "    df[\"previousOwners\"] = pd.to_numeric(df[\"previousOwners\"], errors='coerce').round().astype(\"Int64\")\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors='coerce').round().astype(\"Int64\")\n",
    "    \n",
    "    # Imputation\n",
    "    df = f.fill_NaN_with_categorical(df, \"Brand\", [\"model\",\"transmission\",\"fuelType\"])\n",
    "    df = f.fill_NaN_with_categorical(df, \"Brand\", [\"model\",\"transmission\"])\n",
    "    df = f.fill_NaN_with_categorical(df, \"model\", [\"Brand\",\"transmission\",\"fuelType\"])\n",
    "    df = f.fill_NaN_with_categorical(df, \"model\", [\"Brand\",\"transmission\"])\n",
    "    df = f.fill_NaN_with_categorical(df, \"mpg\", [\"model\",\"fuelType\"])\n",
    "    \n",
    "    df[\"transmission\"] = df[\"transmission\"].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))\n",
    "    df[\"fuelType\"] = df[\"fuelType\"].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))\n",
    "\n",
    "    df = f.fill_NaN_with_mixed(df, \"year\", \"model\", \"mileage\")\n",
    "    df = f.fill_NaN_with_mixed(df, \"mileage\", \"model\", \"year\")\n",
    "    df = f.fill_NaN_with_mixed(df, \"tax\", \"model\", \"year\")\n",
    "    df = f.fill_NaN_with_mixed(df, \"engineSize\", \"model\", \"tax\")\n",
    "\n",
    "    df[\"previousOwners\"] = df[\"previousOwners\"].transform(lambda x: x.fillna(x.median())).round().astype(\"Int64\")\n",
    "    \n",
    "    # Residual Fill\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns.drop([\"carID\", \"price\"], errors='ignore')\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "        global_mean = df[col].median()\n",
    "        df[col] = df[col].fillna(global_mean)\n",
    "        if \"Int64\" in str(df[col].dtype):\n",
    "            df[col] = df[col].round().astype(\"Int64\")\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bda3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar Dados\n",
    "train_db = pd.read_csv(\"./train.csv\")\n",
    "# Importante: Aplicar log no target logo no início se for essa a estratégia\n",
    "train_db['price'] = np.log1p(train_db['price'])\n",
    "\n",
    "# 2. HOLDOUT SPLIT (Separação Inicial)\n",
    "# Aqui garantimos que a validação nunca vê o treino\n",
    "train_set, val_set = train_test_split(train_db, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# 3. Limpeza Independente (Clean Data)\n",
    "train_set = clean_data(train_set)\n",
    "val_set = clean_data(val_set)\n",
    "\n",
    "# 4. ENCODING & SCALING (Onde ocorre o Data Leakage se não tiver cuidado)\n",
    "\n",
    "# A. One-Hot Encoding\n",
    "# Temos de garantir colunas iguais. Concatenamos só para gerar as dummies e separamos de novo.\n",
    "train_len = len(train_set)\n",
    "combined_temp = pd.concat([train_set, val_set], axis=0)\n",
    "\n",
    "combined_temp = pd.get_dummies(combined_temp, columns=[\"Brand\", \"transmission\", \"fuelType\"], drop_first=True)\n",
    "\n",
    "# Separar de volta\n",
    "train_set_encoded = combined_temp.iloc[:train_len].copy()\n",
    "val_set_encoded = combined_temp.iloc[train_len:].copy()\n",
    "\n",
    "# B. Target Encoding (FIT no Treino, TRANSFORM no Treino e Validação)\n",
    "# Calcular médias no Treino\n",
    "mapping = train_set.groupby([\"Brand\", \"model\"])[\"price\"].mean().to_dict()\n",
    "global_mean = train_set[\"price\"].mean()\n",
    "\n",
    "# Aplicar ao Treino\n",
    "train_set_encoded[\"Brand_model_encoded\"] = train_set.apply(\n",
    "    lambda x: mapping.get((x[\"Brand\"], x[\"model\"]), global_mean), axis=1\n",
    ")\n",
    "\n",
    "# Aplicar à Validação (usando o mapping do treino!)\n",
    "val_set_encoded[\"Brand_model_encoded\"] = val_set.apply(\n",
    "    lambda x: mapping.get((x[\"Brand\"], x[\"model\"]), global_mean), axis=1\n",
    ")\n",
    "\n",
    "# 5. PREPARAÇÃO FINAL (X e y)\n",
    "drop_cols = [\"price\", \"carID\", \"model\", \"previousOwners\"] # Colunas a remover\n",
    "\n",
    "X_train = train_set_encoded.drop(columns=drop_cols, errors='ignore')\n",
    "y_train = train_set_encoded[\"price\"]\n",
    "\n",
    "X_val = val_set_encoded.drop(columns=drop_cols, errors='ignore')\n",
    "y_val = val_set_encoded[\"price\"]\n",
    "\n",
    "# C. Scaling (FIT no Treino, TRANSFORM em ambos)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a016a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Juntar os dados processados (Reset index para garantir alinhamento)\n",
    "X_combined = pd.concat([X_train_scaled, X_val_scaled], axis=0).reset_index(drop=True)\n",
    "y_combined = pd.concat([y_train, y_val], axis=0).reset_index(drop=True)\n",
    "\n",
    "# 2. Criar a \"Máscara\" de Split (test_fold)\n",
    "# -1 indica: \"Este dado é de treino, usa para aprender\"\n",
    "#  0 indica: \"Este dado é de validação, usa para testar\" (0 é o índice do fold de validação)\n",
    "\n",
    "# Array com -1 para o tamanho do treino\n",
    "split_index_train = [-1] * len(X_train_scaled)\n",
    "# Array com 0 para o tamanho da validação\n",
    "split_index_val = [0] * len(X_val_scaled)\n",
    "\n",
    "# Juntar os dois\n",
    "test_fold = split_index_train + split_index_val\n",
    "\n",
    "# 3. Criar o Objeto PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configurar o RandomizedSearchCV com cv=ps\n",
    "# Exemplo com Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,    # Número de tentativas\n",
    "    cv=ps,        # <--- AQUI ESTÁ O TRUQUE: Usamos o nosso split personalizado\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5. Executar a Busca\n",
    "print(\"A iniciar Random Search com Predefined Split...\")\n",
    "# Passamos o X_combined e y_combined. Ele vai saber separar internamente graças ao 'ps'.\n",
    "random_search.fit(X_combined, y_combined)\n",
    "\n",
    "# 6. Resultados\n",
    "print(f\"Melhores Parâmetros: {random_search.best_params_}\")\n",
    "print(f\"Melhor RMSE (log): {-random_search.best_score_:.4f}\")\n",
    "\n",
    "# O melhor modelo já fica treinado com TUDO (Treino + Validação) se refit=True (default)\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AVALIAÇÃO DO MODELO NA VALIDAÇÃO ---\n",
    "\n",
    "# 1. Fazer Previsões\n",
    "# Certifica-te que 'best_model' é o nome do teu modelo treinado (ou best_knn, best_rf, etc.)\n",
    "print(\"A calcular previsões na validação...\")\n",
    "y_pred_val_log = best_model.predict(X_val_scaled)\n",
    "\n",
    "# 2. Inverter o Logaritmo (Voltar para Euros)\n",
    "# Como treinaste com np.log1p, tens de usar np.expm1 para reverter\n",
    "y_pred_val_real = np.expm1(y_pred_val_log)\n",
    "y_val_real = np.expm1(y_val) # O y_val original também estava em log\n",
    "\n",
    "# 3. Calcular Métricas\n",
    "r2 = r2_score(y_val_real, y_pred_val_real)\n",
    "mae = mean_absolute_error(y_val_real, y_pred_val_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_real, y_pred_val_real))\n",
    "\n",
    "print(f\"R² Score: {r2:.4f}  (Ideal: 1.0)\")\n",
    "print(f\"MAE:      {mae:.2f} €  (Erro médio em Euros)\")\n",
    "print(f\"RMSE:     {rmse:.2f} €  (Erro penaliza outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c754a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar Teste\n",
    "test_db = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "# 2. Aplicar a mesma Limpeza (Função que criámos antes)\n",
    "# Nota: Garante que definiste a função 'clean_data' no bloco anterior\n",
    "test_db = clean_data(test_db) \n",
    "\n",
    "# 3. ENCODING (Cuidado Máximo aqui!)\n",
    "\n",
    "# A. One-Hot Encoding\n",
    "# O get_dummies no teste pode gerar colunas diferentes. Vamos resolver isso no passo de alinhamento.\n",
    "test_db_encoded = pd.get_dummies(test_db, columns=[\"Brand\", \"transmission\", \"fuelType\"], drop_first=True)\n",
    "\n",
    "# B. Target Encoding\n",
    "# IMPORTANTE: Usar o 'mapping' e 'global_mean' que calculaste no TREINO (não recalcules no teste!)\n",
    "if 'mapping' not in locals() or 'global_mean' not in locals():\n",
    "    raise ValueError(\"Erro: As variáveis 'mapping' e 'global_mean' do treino não estão na memória.\")\n",
    "\n",
    "test_db_encoded[\"Brand_model_encoded\"] = test_db.apply(\n",
    "    lambda x: mapping.get((x[\"Brand\"], x[\"model\"]), global_mean), axis=1\n",
    ")\n",
    "\n",
    "# 4. PREPARAÇÃO FINAL\n",
    "# Remover as mesmas colunas que removemos no treino\n",
    "drop_cols = [\"price\", \"carID\", \"model\", \"previousOwners\"]\n",
    "X_test = test_db_encoded.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# 5. ALINHAMENTO DE COLUNAS (A \"Vacina\" contra erros)\n",
    "# O modelo foi treinado com X_combined. O X_test tem de ter exatamente as mesmas colunas.\n",
    "# Se faltar alguma (ex: Ferrari), preenchemos com 0. Se houver a mais, ignoramos.\n",
    "cols_treino = X_combined.columns # Colunas usadas no fit do RandomizedSearch\n",
    "X_test = X_test.reindex(columns=cols_treino, fill_value=0)\n",
    "\n",
    "# 6. SCALING\n",
    "# Usar o 'scaler' já treinado (não fazer fit!)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# 7. PREVISÃO\n",
    "# Usar o best_model (que já está treinado com tudo)\n",
    "print(\"A fazer previsões finais...\")\n",
    "y_test_pred_log = best_model.predict(X_test_scaled)\n",
    "\n",
    "# 8. INVERTER O LOG (Trazer de volta para Euros)\n",
    "y_test_pred_real = np.expm1(y_test_pred_log)\n",
    "\n",
    "# 9. GUARDAR SUBMISSÃO\n",
    "submission = pd.DataFrame({\n",
    "    \"carID\": test_db[\"carID\"], # Buscar o ID original\n",
    "    \"price\": y_test_pred_real\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_final_campeao.csv\", index=False)\n",
    "print(\"Sucesso! Ficheiro 'submission_final_campeao.csv' criado.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
